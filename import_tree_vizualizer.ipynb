{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility to visuzalize various imports in any module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text, mode=''):\n",
    "    \"\"\"\n",
    "    mode: 'debug' to print intermediate steps\n",
    "    \"\"\"\n",
    "    coding = {}\n",
    "    for line in text.split('\\n'):\n",
    "        base = line.split(' ')\n",
    "        if mode=='debug':\n",
    "            print(base)\n",
    "            \n",
    "        if base[0] == 'import':\n",
    "            key = base[1]\n",
    "            if len(base)==4 and base[2]=='as':\n",
    "                alias = base[3]\n",
    "            else:\n",
    "                alias = ''\n",
    "            coding[key] = {'alias':alias if alias else None}\n",
    "        elif base[0]=='from':\n",
    "            child = None\n",
    "            if '.' in base[1]:\n",
    "                temp  = base[1].split('.')\n",
    "                key   = temp[0]\n",
    "                child = base[1]\n",
    "                if mode=='debug':\n",
    "                    print(\"child\",child)\n",
    "            else:\n",
    "                key   = base[1]\n",
    "            for indx, t in enumerate(base):\n",
    "                if t!='import':\n",
    "                    continue\n",
    "                if t=='import':\n",
    "                    imported = ' '.join(base[(indx+1):])\n",
    "                    if mode=='debug':\n",
    "                        print(\"imported pre\", imported)\n",
    "                    imported = ','.join([x for x in imported.split(',') if x!=''])\n",
    "            if mode=='debug':\n",
    "                if mode=='debug':\n",
    "                    print(\"imported\", imported)\n",
    "            if key in coding and 'import' in coding[key] and coding[key]['import']:\n",
    "                if child in coding[key]['import']:\n",
    "                    coding[key]['import'][child].append(imported)\n",
    "                else:\n",
    "                    coding[key]['import'][child] = [imported]\n",
    "            else:\n",
    "                coding[key] = {'alias': None, }\n",
    "                if child:\n",
    "                    coding[key]['import'] =  {child: [imported]}\n",
    "    return coding\n",
    "\n",
    "def vizualize_coding(coding):\n",
    "    for key, value in coding.items():\n",
    "        if value['alias'] is not None:\n",
    "            print(key, \"->\", value['alias'])\n",
    "        else:\n",
    "            print(key)\n",
    "        if 'import' in value:\n",
    "            level=1\n",
    "            for key, value in value['import'].items():\n",
    "                print(\"└───\",key)\n",
    "                print(\"\\t\"+\"└───\", ','.join(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'pyspark': {'alias': None, 'import': {'pyspark.sql': ['SparkSession'], 'pyspark.sql.functions': ['when, col, row_number,x'], 'pyspark.sql.window': ['Window as W, Split as S', 'Window as W, Split '], 'pyspark.sql.types': ['IntegerType, FloatType']}}, 'pyspark.sql.functions': {'alias': 'F'}, 'pandas': {'alias': 'pd'}, 're': {'alias': None}}\n",
      "\n",
      "pyspark\n",
      "└─── pyspark.sql\n",
      "\t└─── SparkSession\n",
      "└─── pyspark.sql.functions\n",
      "\t└─── when, col, row_number,x\n",
      "└─── pyspark.sql.window\n",
      "\t└─── Window as W, Split as S,Window as W, Split \n",
      "└─── pyspark.sql.types\n",
      "\t└─── IntegerType, FloatType\n",
      "pyspark.sql.functions -> F\n",
      "pandas -> pd\n",
      "re\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import when, col, row_number,x\n",
    "from pyspark.sql.window import Window as W, Split as S\n",
    "from pyspark.sql.window import Window as W, Split \n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "coding = parse_text(text)\n",
    "print()\n",
    "print(coding)\n",
    "print()\n",
    "\n",
    "vizualize_coding(coding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn: linear models example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "numbers\n",
      "warnings\n",
      "numpy -> np\n",
      "scipy\n",
      "joblib\n",
      "\n",
      "└─── ._base\n",
      "\t└─── LinearClassifierMixin, SparseCoefMixin, BaseEstimator\n",
      "└─── ._linear_loss\n",
      "\t└─── LinearModelLoss\n",
      "└─── ._sag\n",
      "\t└─── sag_solver\n",
      "└─── .._loss.loss\n",
      "\t└─── HalfBinomialLoss, HalfMultinomialLoss\n",
      "└─── ..preprocessing\n",
      "\t└─── LabelEncoder, LabelBinarizer\n",
      "└─── ..svm._base\n",
      "\t└─── _fit_liblinear\n",
      "└─── ..utils\n",
      "\t└─── check_array, check_consistent_length, compute_class_weight,check_random_state\n",
      "└─── ..utils.extmath\n",
      "\t└─── softmax,row_norms\n",
      "└─── ..utils.optimize\n",
      "\t└─── _newton_cg, _check_optimize_result\n",
      "└─── ..utils.validation\n",
      "\t└─── check_is_fitted, _check_sample_weight\n",
      "└─── ..utils.multiclass\n",
      "\t└─── check_classification_targets\n",
      "└─── ..utils.fixes\n",
      "\t└─── delayed\n",
      "└─── ..model_selection\n",
      "\t└─── check_cv\n",
      "└─── ..metrics\n",
      "\t└─── get_scorer\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/scikit-learn/scikit-learn/tree/main/sklearn/linear_model\n",
    "text_sklearn = \"\"\"import numbers\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from joblib import Parallel, effective_n_jobs\n",
    "\n",
    "from ._base import LinearClassifierMixin, SparseCoefMixin, BaseEstimator\n",
    "from ._linear_loss import LinearModelLoss\n",
    "from ._sag import sag_solver\n",
    "from .._loss.loss import HalfBinomialLoss, HalfMultinomialLoss\n",
    "from ..preprocessing import LabelEncoder, LabelBinarizer\n",
    "from ..svm._base import _fit_liblinear\n",
    "from ..utils import check_array, check_consistent_length, compute_class_weight\n",
    "from ..utils import check_random_state\n",
    "from ..utils.extmath import softmax\n",
    "from ..utils.extmath import row_norms\n",
    "from ..utils.optimize import _newton_cg, _check_optimize_result\n",
    "from ..utils.validation import check_is_fitted, _check_sample_weight\n",
    "from ..utils.multiclass import check_classification_targets\n",
    "from ..utils.fixes import delayed\n",
    "from ..model_selection import check_cv\n",
    "from ..metrics import get_scorer\"\"\"\n",
    "\n",
    "coding_sklearn = parse_text(text_sklearn, mode='')\n",
    "print()\n",
    "# print(coding_sklearn)\n",
    "print()\n",
    "\n",
    "vizualize_coding(coding_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn: ridge models example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "abc\n",
      "functools\n",
      "warnings\n",
      "numpy -> np\n",
      "numbers\n",
      "scipy\n",
      "└─── scipy.sparse\n",
      "\t└─── linalg as sp_linalg\n",
      "\n",
      "└─── ._base\n",
      "\t└─── LinearClassifierMixin, LinearModel,_deprecate_normalize, _preprocess_data, _rescale_data\n",
      "└─── ._sag\n",
      "\t└─── sag_solver\n",
      "└─── ..base\n",
      "\t└─── MultiOutputMixin, RegressorMixin, is_classifier\n",
      "└─── ..utils.extmath\n",
      "\t└─── safe_sparse_dot,row_norms\n",
      "└─── ..utils\n",
      "\t└─── check_array,check_consistent_length,check_scalar,compute_sample_weight,column_or_1d\n",
      "└─── ..utils.validation\n",
      "\t└─── check_is_fitted,_check_sample_weight\n",
      "└─── ..preprocessing\n",
      "\t└─── LabelBinarizer\n",
      "└─── ..model_selection\n",
      "\t└─── GridSearchCV\n",
      "└─── ..metrics\n",
      "\t└─── check_scoring\n",
      "└─── ..exceptions\n",
      "\t└─── ConvergenceWarning\n",
      "└─── ..utils.sparsefuncs\n",
      "\t└─── mean_variance_axis\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/linear_model/_ridge.py\n",
    "\n",
    "text_ridge=\"\"\"from abc import ABCMeta, abstractmethod\n",
    "from functools import partial\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import numbers\n",
    "from scipy import linalg\n",
    "from scipy import sparse\n",
    "from scipy import optimize\n",
    "from scipy.sparse import linalg as sp_linalg\n",
    "\n",
    "from ._base import LinearClassifierMixin, LinearModel\n",
    "from ._base import _deprecate_normalize, _preprocess_data, _rescale_data\n",
    "from ._sag import sag_solver\n",
    "from ..base import MultiOutputMixin, RegressorMixin, is_classifier\n",
    "from ..utils.extmath import safe_sparse_dot\n",
    "from ..utils.extmath import row_norms\n",
    "from ..utils import check_array\n",
    "from ..utils import check_consistent_length\n",
    "from ..utils import check_scalar\n",
    "from ..utils import compute_sample_weight\n",
    "from ..utils import column_or_1d\n",
    "from ..utils.validation import check_is_fitted\n",
    "from ..utils.validation import _check_sample_weight\n",
    "from ..preprocessing import LabelBinarizer\n",
    "from ..model_selection import GridSearchCV\n",
    "from ..metrics import check_scoring\n",
    "from ..exceptions import ConvergenceWarning\n",
    "from ..utils.sparsefuncs import mean_variance_axis\n",
    "\"\"\"\n",
    "\n",
    "coding_ridge = parse_text(text_ridge,mode='')\n",
    "print()\n",
    "#print(coding_ridge)\n",
    "print()\n",
    "\n",
    "vizualize_coding(coding_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "inspect\n",
      "sys\n",
      "functools\n",
      "warnings\n",
      "typing\n",
      "pyspark\n",
      "└─── pyspark.rdd\n",
      "\t└─── PythonEvalType\n",
      "└─── pyspark.sql.column\n",
      "\t└─── Column, _to_java_column, _to_seq, _create_column_from_literal\n",
      "└─── pyspark.sql.dataframe\n",
      "\t└─── DataFrame\n",
      "└─── pyspark.sql.types\n",
      "\t└─── ArrayType, DataType, StringType, StructType\n",
      "└─── pyspark.sql.udf\n",
      "\t└─── UserDefinedFunction, _create_udf  # noqa: F401\n",
      "└─── pyspark.sql.pandas.functions\n",
      "\t└─── pandas_udf, PandasUDFType  # noqa: F401\n",
      "└─── pyspark.sql.utils\n",
      "\t└─── to_str\n"
     ]
    }
   ],
   "source": [
    "text_pyspark = \"\"\"import inspect\n",
    "import sys\n",
    "import functools\n",
    "import warnings\n",
    "from typing import (\n",
    "    Any,\n",
    "    cast,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    List,\n",
    "    Iterable,\n",
    "    overload,\n",
    "    Optional,\n",
    "    Tuple,\n",
    "    TYPE_CHECKING,\n",
    "    Union,\n",
    "    ValuesView,\n",
    ")\n",
    "\n",
    "from pyspark import since, SparkContext\n",
    "from pyspark.rdd import PythonEvalType\n",
    "from pyspark.sql.column import Column, _to_java_column, _to_seq, _create_column_from_literal\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.types import ArrayType, DataType, StringType, StructType\n",
    "\n",
    "# Keep UserDefinedFunction import for backwards compatible import; moved in SPARK-22409\n",
    "from pyspark.sql.udf import UserDefinedFunction, _create_udf  # noqa: F401\n",
    "\n",
    "# Keep pandas_udf and PandasUDFType import for backwards compatible import; moved in SPARK-28264\n",
    "from pyspark.sql.pandas.functions import pandas_udf, PandasUDFType  # noqa: F401\n",
    "from pyspark.sql.utils import to_str\n",
    "\"\"\"\n",
    "\n",
    "coding_pyspark = parse_text(text_pyspark,mode='')\n",
    "print()\n",
    "#print(coding_pyspark)\n",
    "print()\n",
    "\n",
    "vizualize_coding(coding_pyspark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Need to Fix issue when () and import on next line # like in typing\n",
    "2. Need to ignore comments (# noqa: F401)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
